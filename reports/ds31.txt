https://www.kimi.com/chat/d3a6qros8jduhn0uutfg 

ResearchParÃ§a is a full-stack web application that turns a raw pile of academic PDFs and BibTeX files into an interactive, filterable, â€œspreadsheet-meets-dashboardâ€ for systematic literature reviews.  
It was built for one concrete problem â€“ a masterâ€™s thesis on automatic PCB (printed-circuit board) visual inspection â€“ but every design choice is generic enough to let any researcher drop in a bibliography and immediately start curating, classifying and visualising hundreds of papers.

Below is a walk-through of what the tool actually *does*, grouped by the moment the user meets each capability.

--------------------------------------------------
1. First contact â€“ â€œI just want to see the papersâ€
--------------------------------------------------
â€¢ One-command start  
  `python browse_db.py my_db.sqlite` opens a self-contained Flask server and, by default, launches the browser.  
  No Docker, no npm, no external CDN is ever contacted; fonts, JS, CSS and even the compressed HTML export are served locally.

â€¢ Instant table with frozen columns  
  The landing page is a wide table whose first five columns (type, title, year, journal, pages) stay on screen while the rest scrolls.  
  Every header is clickable for ascending/descending sort; repeated clicks cycle ASC â†’ DESC â†’ none.

â€¢ Emoji language  
  All boolean or tri-state fields are shown with single Unicode glyphs:  
  âœ”ï¸ = yes, âŒ = no, â” = unknown, ğŸ‘¤ = human-edited, ğŸ–¥ï¸ = machine-edited.  
  Hovering reveals the plain-English meaning; colour-blind users still see the symbol.

--------------------------------------------------
2. Search & slice â€“ â€œShow me onlyâ€¦â€
--------------------------------------------------
â€¢ Omnisearch  
  A single search box performs a case-insensitive AND search across title, authors, keywords, abstract, DOI, journal, research-area tag and user comments.  
  Matches are highlighted in real time; the table updates in < 200 ms for 15 k records on a laptop.

â€¢ Server-side filters  
  Year range, minimum page count and an â€œoff-topicâ€ switch are pushed to the SQL layer so that large collections do not reach the browser.

â€¢ Client-side toggles  
  Dozens of check-boxes above the table hide or show whole families of papers (X-ray studies, surveys-only, PCB-versus-PCBA focus, etc.).  
  Because filtering happens in the browser, toggles feel instantaneous and do not reload the page.

â€¢ Persistent URL  
  Every filter state is encoded in the query string; copying the URL gives a colleague the same slice of the corpus.

--------------------------------------------------
3. Curate â€“ â€œThis one is wrongly labelledâ€
--------------------------------------------------
â€¢ Click-to-cycle editing  
  Any âœ”ï¸/âŒ/â” cell can be clicked; the symbol rotates and the new value is PATCHed to the server in the background.  
  A pale-yellow flash confirms the save; no â€œsubmitâ€ button is ever needed.

â€¢ Row-detail drawer  
  Clicking â€œShowâ€ unfolds a sub-row that contains the full abstract, keywords, DOI link and an editable mini-form (research area, model name, free-text â€œother defectsâ€, user comment).  
  Changes inside the drawer are explicitly saved with a â€œSave Changesâ€ button to avoid half-finished edits.

â€¢ Audit trail  
  Every mutation writes a UTC timestamp and the actor (â€œuserâ€ or the running LLM alias).  
  The last editor and date are visible in the table so that supervisors can see which rows still need human eyes.

--------------------------------------------------
4. Bulk AI classification â€“ â€œLet the machine do the grunt workâ€
--------------------------------------------------
â€¢ Two-stage pipeline  
  1. *Classifier* â€“ reads title + abstract and outputs a JSON ballot (is_survey, is_through_hole, features.solder_void, technique.dl_cnn_detector, â€¦).  
  2. *Verifier* â€“ reads the same text plus the ballot and returns a second JSON with a confidence score and a â€œverifiedâ€ flag; if the verifier disagrees, both traces are stored.

â€¢ Local inference  
  Both stages are served by a single llama.cpp endpoint (default http://localhost:8080).  
  Any OpenAI-compatible server works; the code auto-detects the model name and stores it for traceability.

â€¢ Granular control  
  â€“ Classify/verify one paper from its drawer.  
  â€“ Classify/verify â€œremainingâ€ (papers without machine ballots).  
  â€“ Re-classify or re-verify *all* papers (destructive, asks for confirmation).  
  Each batch runs in a background thread; the UI stays responsive and shows a small status banner.

â€¢ Interpretability  
  The chain-of-thought text produced by the model is saved verbatim and displayed in the drawer, letting a human referee copy-paste the rationale or spot hallucinations.

--------------------------------------------------
5. Statistics & exploration â€“ â€œWhat does the corpus look like?â€
--------------------------------------------------
â€¢ Repeating-entities lists  
  Journals, conferences, keywords, authors and research areas are counted on the *currently filtered* subset and shown as ranked lists with scrollable multi-column layout.

â€¢ Feature & technique distributions  
  Horizontal bar charts show how many papers deal with â€œsolder voidsâ€, â€œCNN detectorsâ€, etc.  
  The same counts are broken down by publication year in interactive line charts (Chart.js).

â€¢ Time evolution  
  Extra line charts track:  
  â€“ Survey papers versus implementation papers per year;  
  â€“ Publication type mix (journal, conference, arXiv, etc.) per year;  
  â€“ Any custom field the user added to the JSON schema.

â€¢ Live update  
  Pressing the â€œView Statisticsâ€ button recomputes every chart from the visible rows, so filtering for â€œafter 2020â€ or â€œonly IEEEâ€ immediately redraws the trends.

--------------------------------------------------
6. Import / export â€“ â€œTake the data with youâ€
--------------------------------------------------
â€¢ BibTeX ingestion  
  A hidden `<input type=file>` accepts `.bib` files; the backend parses them with pybtex, normalises author names, guesses page counts and inserts new rows without touching existing manual edits.

â€¢ Single-file HTML snapshot  
  â€œExport HTMLâ€ produces a self-contained, gzipped, base-64 encoded page that opens in any browser even offline.  
  All CSS/JS is inlined; the snapshot remains searchable and filterable via the same client-side code, but editing is disabled.

â€¢ Excel workbook  
  â€œExport XLSXâ€ streams an .xlsx file with one row per paper and one column per boolean or text field.  
  Conditional formatting paints TRUE cells light-green and FALSE cells light-red, ready for pivot tables or VLOOKUP gymnastics in Excel/LibreOffice.

--------------------------------------------------
7. Non-functional niceties
--------------------------------------------------
â€¢ Keyboard accessibility  
  Every interactive element is reachable via Tab; Enter triggers the default action; Escape closes modals.

â€¢ Responsive cues  
  Although the table is intrinsically wide, the header collapses into two rows on narrow screens and the font-size scales slightly so that 13-inch laptops do not horizontal-scroll by default.

â€¢ Performance guard-rails  
  â€“ FTS5 virtual tables in SQLite give millisecond search even when the corpus grows past 50 k papers.  
  â€“ Row details are loaded lazily (one AJAX call per row) so the initial HTML payload stays under ~200 kB for 1 k papers.  
  â€“ All JavaScript is vanilla; no framework re-renders keep the browserâ€™s main thread free for smooth scrolling.

--------------------------------------------------
In one sentence  
ResearchParÃ§a is a turnkey, offline-first â€œspreadsheet + BI toolkit + AI internâ€ that lets a researcher move from â€œa messy folder of PDFsâ€ to â€œan interactive, shareable, fully classified and charted literature reviewâ€ without ever leaving the browser.
